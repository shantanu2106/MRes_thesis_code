{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477d6377",
   "metadata": {},
   "source": [
    "The data was collected for each person in the sample iteratively and stored in the zip file. Before the operation, the data needs to be transformed into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f990adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_zip_files(source_folder, destination_folder):\n",
    "    source_path = Path(source_folder)\n",
    "    destination_path = Path(destination_folder)\n",
    "    destination_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    total_files_collected = 0\n",
    "    \n",
    "    for subfolder in source_path.iterdir():\n",
    "        if subfolder.is_dir():\n",
    "            zip_files = list(subfolder.glob('*.zip')) + list(subfolder.glob('*.ZIP'))\n",
    "            \n",
    "            for zip_file in zip_files:\n",
    "                original_name = zip_file.name\n",
    "                destination_file = destination_path / original_name\n",
    "                \n",
    "                counter = 1\n",
    "                base_name = zip_file.stem\n",
    "                extension = zip_file.suffix\n",
    "                \n",
    "                while destination_file.exists():\n",
    "                    new_name = f\"{base_name}_{counter}{extension}\"\n",
    "                    destination_file = destination_path / new_name\n",
    "                    counter += 1\n",
    "                \n",
    "                shutil.copy2(zip_file, destination_file)\n",
    "                total_files_collected += 1\n",
    "    \n",
    "    print(f\"Total zip files collected: {total_files_collected}\")\n",
    "\n",
    "source_folder = \"/Users/shantanusharma/Desktop/untitled folder 2\"\n",
    "destination_folder = \"/Users/shantanusharma/Desktop/Data Coolection Docs\"\n",
    "collect_zip_files(source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac521445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx_file(file_path: str) -> str:\n",
    "    \"\"\"Read a .docx file and convert it to plain text.\"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                full_text.append(paragraph.text)\n",
    "        return '\\n'.join(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def extract_document_components(document_text: str) -> Dict[str, any]:\n",
    "    \"\"\"Extract source name, body text, and year from document.\"\"\"\n",
    "    # Extract source name\n",
    "    lines = document_text.split('\\n')\n",
    "    source_name = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line and i + 1 < len(lines):\n",
    "            next_line = lines[i + 1].strip()\n",
    "            if re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December).*\\d{4}', next_line):\n",
    "                if not line.startswith('#') and not line.startswith('!') and not line.isupper():\n",
    "                    source_name = line\n",
    "                    break\n",
    "    \n",
    "    # Extract body text\n",
    "    body_start_pattern = r'(?:Body|\\*\\*Body\\*\\*|__Body__|Section:\\s*Body)'\n",
    "    body_end_pattern = r'(?:Load-Date:|Load Date:|__Load-Date__|Load-Date)'\n",
    "    \n",
    "    body_match = re.search(f'{body_start_pattern}(.*?){body_end_pattern}', \n",
    "                          document_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if body_match:\n",
    "        body_text = body_match.group(1).strip()\n",
    "    else:\n",
    "        fallback_match = re.search(r'Byline:.*?\\n(.*?)(?:Load-Date|Load Date)', \n",
    "                                  document_text, re.DOTALL | re.IGNORECASE)\n",
    "        body_text = fallback_match.group(1).strip() if fallback_match else \"\"\n",
    "    \n",
    "    # Clean body text\n",
    "    body_text = re.sub(r'\\*\\*|\\[\\]{\\.underline}|\\{\\.underline\\}', '', body_text)\n",
    "    body_text = re.sub(r'\\s+', ' ', body_text).strip()\n",
    "    \n",
    "    # Extract year\n",
    "    year_patterns = [\n",
    "        r'Load-Date:.*?(\\d{4})',\n",
    "        r'Load Date:.*?(\\d{4})',\n",
    "        r'__Load-Date__.*?(\\d{4})',\n",
    "        r'(\\d{4})\\s*End of Document'\n",
    "    ]\n",
    "    \n",
    "    year = None\n",
    "    for pattern in year_patterns:\n",
    "        year_match = re.search(pattern, document_text, re.IGNORECASE)\n",
    "        if year_match:\n",
    "            year = int(year_match.group(1))\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'source_name': source_name,\n",
    "        'body_text': body_text,\n",
    "        'year': year\n",
    "    }\n",
    "\n",
    "def extract_person_sentences_with_context(text: str, last_name: str, nlp) -> str:\n",
    "    \"\"\"Extract sentences containing the person's name with context.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create name variations\n",
    "    name_variations = [last_name]\n",
    "    name_variations.extend([f\"Mr. {last_name}\", f\"Ms. {last_name}\", \n",
    "                           f\"Mrs. {last_name}\", f\"Dr. {last_name}\"])\n",
    "    \n",
    "    relevant_text_parts = []\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent_text = sent.text.strip()\n",
    "        if any(name.lower() in sent_text.lower() for name in name_variations):\n",
    "            # Add previous sentence if exists\n",
    "            if i > 0:\n",
    "                relevant_text_parts.append(sentences[i-1].text.strip())\n",
    "            # Add current sentence\n",
    "            relevant_text_parts.append(sent_text)\n",
    "            # Add next sentence if exists\n",
    "            if i < len(sentences) - 1:\n",
    "                relevant_text_parts.append(sentences[i+1].text.strip())\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    relevant_text_cleaned = []\n",
    "    for text in relevant_text_parts:\n",
    "        if text not in seen:\n",
    "            seen.add(text)\n",
    "            relevant_text_cleaned.append(text)\n",
    "    \n",
    "    return ' '.join(relevant_text_cleaned) if relevant_text_cleaned else \"\"\n",
    "\n",
    "# ===== TEXT PREPROCESSING =====\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess text according to specified steps.\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Convert special characters to ASCII equivalents\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # 3. Remove words starting with @ or #\n",
    "    text = re.sub(r'[@#]\\w+', '', text)\n",
    "    \n",
    "    # 4. Remove non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # 5. Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 6. Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 7. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 8. Remove extra punctuation\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def collect_all_ceo_documents(main_folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect all documents from CEO subfolders and create initial dataframe.\n",
    "    \n",
    "    Args:\n",
    "        main_folder_path: Path to main folder containing CEO subfolders\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: CEO_Name, Source, Year, Body_Text, File_Path\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Get all subfolders (CEO folders)\n",
    "    ceo_folders = [f for f in os.listdir(main_folder_path) \n",
    "                   if os.path.isdir(os.path.join(main_folder_path, f))]\n",
    "    \n",
    "    print(f\"Found {len(ceo_folders)} CEO folders\")\n",
    "    \n",
    "    # Process each CEO folder\n",
    "    for ceo_folder in tqdm(ceo_folders, desc=\"Processing CEO folders\"):\n",
    "        # Extract CEO name from folder name\n",
    "        ceo_name = ceo_folder.replace('_', ' ')\n",
    "        \n",
    "        ceo_folder_path = os.path.join(main_folder_path, ceo_folder)\n",
    "        \n",
    "        # Get all .docx files in the folder\n",
    "        docx_files = [f for f in os.listdir(ceo_folder_path) \n",
    "                     if f.lower().endswith('.docx') and not f.startswith('~')]\n",
    "        \n",
    "        print(f\"\\n{ceo_name}: Found {len(docx_files)} documents\")\n",
    "        \n",
    "        # Process each document\n",
    "        for docx_file in docx_files:\n",
    "            file_path = os.path.join(ceo_folder_path, docx_file)\n",
    "            \n",
    "            # Read document\n",
    "            doc_text = read_docx_file(file_path)\n",
    "            \n",
    "            if not doc_text:\n",
    "                continue\n",
    "            \n",
    "            # Extract components\n",
    "            components = extract_document_components(doc_text)\n",
    "            \n",
    "            # Add to list\n",
    "            all_documents.append({\n",
    "                'CEO_Name': ceo_name,\n",
    "                'Source': components['source_name'],\n",
    "                'Year': components['year'],\n",
    "                'Body_Text': components['body_text'],\n",
    "                'File_Path': file_path,\n",
    "                'Filename': docx_file\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_raw = pd.DataFrame(all_documents)\n",
    "    \n",
    "    print(f\"\\nTotal documents collected: {len(df_raw)}\")\n",
    "    print(f\"CEOs with data: {df_raw['CEO_Name'].nunique()}\")\n",
    "    print(f\"Sources found: {df_raw['Source'].nunique()}\")\n",
    "    print(f\"Year range: {df_raw['Year'].min()} - {df_raw['Year'].max()}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "def create_ner_extracted_dataframe(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply NER extraction with context to create extracted text dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df_raw: Raw dataframe with body text\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with NER-extracted text\n",
    "    \"\"\"\n",
    "    print(\"\\nApplying NER extraction with context...\")\n",
    "    \n",
    "    extracted_data = []\n",
    "    \n",
    "    # Group by CEO for efficient processing\n",
    "    for ceo_name, group in tqdm(df_raw.groupby('CEO_Name'), desc=\"Processing CEOs\"):\n",
    "        # Get last name for NER\n",
    "        last_name = ceo_name.split()[-1]\n",
    "        \n",
    "        for idx, row in group.iterrows():\n",
    "            # Extract relevant sentences with context\n",
    "            extracted_text = extract_person_sentences_with_context(\n",
    "                row['Body_Text'], last_name, nlp\n",
    "            )\n",
    "            \n",
    "            if extracted_text:  # Only add if text was found\n",
    "                extracted_data.append({\n",
    "                    'CEO_Name': ceo_name,\n",
    "                    'Source': row['Source'],\n",
    "                    'Year': row['Year'],\n",
    "                    'Extracted_Text': extracted_text,\n",
    "                    'Filename': row['Filename']\n",
    "                })\n",
    "    \n",
    "    df_extracted = pd.DataFrame(extracted_data)\n",
    "    \n",
    "    print(f\"\\nNER extraction complete!\")\n",
    "    print(f\"Documents with extracted text: {len(df_extracted)}\")\n",
    "    print(f\"Documents without matches: {len(df_raw) - len(df_extracted)}\")\n",
    "    \n",
    "    return df_extracted\n",
    "\n",
    "def create_preprocessed_dataframe(df_extracted: pd.DataFrame, min_words: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create preprocessed version of extracted text dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df_extracted: DataFrame with NER-extracted text\n",
    "        min_words: Minimum word count threshold\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    print(\"\\nPreprocessing extracted text...\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    df_preprocessed = df_extracted.copy()\n",
    "    df_preprocessed['Preprocessed_Text'] = df_preprocessed['Extracted_Text'].apply(preprocess_text)\n",
    "    \n",
    "    # Calculate word count\n",
    "    df_preprocessed['Word_Count'] = df_preprocessed['Preprocessed_Text'].apply(\n",
    "        lambda x: len(str(x).split()) if x else 0\n",
    "    )\n",
    "    \n",
    "    # Filter by word count\n",
    "    initial_count = len(df_preprocessed)\n",
    "    df_preprocessed = df_preprocessed[df_preprocessed['Word_Count'] >= min_words].copy()\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Documents before filtering: {initial_count}\")\n",
    "    print(f\"Documents after {min_words}-word filter: {len(df_preprocessed)}\")\n",
    "    print(f\"Average word count: {df_preprocessed['Word_Count'].mean():.1f}\")\n",
    "    \n",
    "    # Remove word count column if not needed\n",
    "    df_preprocessed = df_preprocessed.drop('Word_Count', axis=1)\n",
    "    \n",
    "    return df_preprocessed\n",
    "\n",
    "def run_data_collection_pipeline(main_folder_path: str, output_folder: str = \"output\") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run the complete data collection pipeline.\n",
    "    \n",
    "    Args:\n",
    "        main_folder_path: Path to main folder containing CEO subfolders\n",
    "        output_folder: Folder to save output files\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (raw_df, extracted_df, preprocessed_df)\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING CEO DATA COLLECTION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Collect all documents\n",
    "    print(\"\\nSTEP 1: Collecting all documents...\")\n",
    "    df_raw = collect_all_ceo_documents(main_folder_path)\n",
    "    \n",
    "    # Save raw dataframe\n",
    "    raw_file = os.path.join(output_folder, \"ceo_articles_raw.csv\")\n",
    "    df_raw.to_csv(raw_file, index=False)\n",
    "    print(f\"Raw data saved to: {raw_file}\")\n",
    "    \n",
    "    # Step 2: Apply NER extraction\n",
    "    print(\"\\nSTEP 2: Applying NER extraction...\")\n",
    "    df_extracted = create_ner_extracted_dataframe(df_raw)\n",
    "    \n",
    "    # Save extracted dataframe\n",
    "    extracted_file = os.path.join(output_folder, \"ceo_articles_extracted.csv\")\n",
    "    df_extracted.to_csv(extracted_file, index=False)\n",
    "    print(f\"Extracted data saved to: {extracted_file}\")\n",
    "    \n",
    "    # Step 3: Preprocess text\n",
    "    print(\"\\nSTEP 3: Preprocessing text...\")\n",
    "    df_preprocessed = create_preprocessed_dataframe(df_extracted)\n",
    "    \n",
    "    # Save preprocessed dataframe\n",
    "    preprocessed_file = os.path.join(output_folder, \"ceo_articles_preprocessed.csv\")\n",
    "    df_preprocessed.to_csv(preprocessed_file, index=False)\n",
    "    print(f\"Preprocessed data saved to: {preprocessed_file}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE COMPLETE - SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total CEOs processed: {df_preprocessed['CEO_Name'].nunique()}\")\n",
    "    print(f\"Total articles in final dataset: {len(df_preprocessed)}\")\n",
    "    print(f\"Sources in dataset: {df_preprocessed['Source'].nunique()}\")\n",
    "    print(f\"Year range: {df_preprocessed['Year'].min()} - {df_preprocessed['Year'].max()}\")\n",
    "    \n",
    "    # Show CEO article distribution\n",
    "    print(\"\\nArticles per CEO (top 10):\")\n",
    "    ceo_counts = df_preprocessed['CEO_Name'].value_counts().head(10)\n",
    "    for ceo, count in ceo_counts.items():\n",
    "        print(f\"  {ceo}: {count}\")\n",
    "    \n",
    "    return df_raw, df_extracted, df_preprocessed\n",
    "\n",
    "# ===== USAGE =====\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your main folder path\n",
    "    main_folder_path = \"/Users/shantanusharma/Desktop/Docs_for_analysis\"  # The path where the zip files would be located\n",
    "    \n",
    "    # Run the pipeline\n",
    "    df_raw, df_extracted, df_preprocessed = run_data_collection_pipeline(\n",
    "        main_folder_path,\n",
    "        output_folder= \"ceo_analysis_output\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
