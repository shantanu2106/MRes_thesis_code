{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68fbc86",
   "metadata": {},
   "source": [
    "Running the Topic Modelling and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LdaModel, LsiModel, Nmf, CoherenceModel\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = '/Users/shantanusharma/Desktop/preprocessed_sentences_final.csv'\n",
    "OUTPUT_DIR = 'topic_model_outputs'\n",
    "NUM_REPRESENTATIVE_SENTENCES = 10\n",
    "NUM_TOPIC_WORDS = 15\n",
    "\n",
    "# --- Create Output Directories ---\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'lda_results')):\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, 'lda_results'))\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'validation_models')):\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, 'validation_models'))\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{INPUT_FILE}' not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "if df.empty:\n",
    "    print(\"No data in the preprocessed DataFrame. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "all_preprocessed_sentences_tokenized = [s.split() for s in df['preprocessed_tokens_str'].tolist()]\n",
    "original_sentence_col = 'original_sentence'\n",
    "\n",
    "# --- Prepare Gensim Corpus ---\n",
    "print(\"Preparing Gensim corpus...\")\n",
    "dictionary = Dictionary(all_preprocessed_sentences_tokenized)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.9, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in all_preprocessed_sentences_tokenized]\n",
    "tfidf_model = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "\n",
    "# =========================================================================== #\n",
    "# 1. INTERACTIVE MODEL TUNING\n",
    "# =========================================================================== #\n",
    "print(\"\\n--- Starting Topic Model Tuning ---\")\n",
    "\n",
    "def calculate_exclusivity(model, dictionary, top_n_words=10):\n",
    "    \"\"\"Calculates topic exclusivity.\"\"\"\n",
    "    exclusivity_scores = []\n",
    "    topics = model.get_topics()\n",
    "    for topic_index in range(model.num_topics):\n",
    "        top_word_indices = np.argsort(topics[topic_index])[-top_n_words:]\n",
    "        topic_exclusivity = 0\n",
    "        for word_index in top_word_indices:\n",
    "            p_word_in_topic = topics[topic_index, word_index]\n",
    "            p_word_in_all_topics = np.sum(topics[:, word_index])\n",
    "            if p_word_in_all_topics > 1e-9:\n",
    "                topic_exclusivity += p_word_in_topic / p_word_in_all_topics\n",
    "        exclusivity_scores.append(topic_exclusivity / top_n_words)\n",
    "    return np.mean(exclusivity_scores)\n",
    "\n",
    "topic_range = range(5, 31, 5)\n",
    "coherence_scores = []\n",
    "exclusivity_scores = []\n",
    "\n",
    "for k in topic_range:\n",
    "    print(f\"Training and evaluating model with {k} topics...\")\n",
    "    lda_model_temp = LdaModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=k, passes=15, random_state=42)\n",
    "    coherence_model = CoherenceModel(model=lda_model_temp, texts=all_preprocessed_sentences_tokenized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_scores.append(coherence_model.get_coherence())\n",
    "    exclusivity_scores.append(calculate_exclusivity(lda_model_temp, dictionary))\n",
    "\n",
    "# --- Plotting with Dual-Axis Line Graph ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "color1 = 'tab:blue'\n",
    "ax1.set_xlabel('Number of Topics', fontsize=12)\n",
    "ax1.set_ylabel('Topic Coherence (c_v)', color=color1, fontsize=12)\n",
    "ax1.plot(topic_range, coherence_scores, 'o-', color=color1, label='Coherence')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.grid(axis='x')\n",
    "for i, txt in enumerate(coherence_scores):\n",
    "    ax1.annotate(f'{txt:.3f}', (list(topic_range)[i], coherence_scores[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'tab:red'\n",
    "ax2.set_ylabel('Semantic Exclusivity', color=color2, fontsize=12)\n",
    "ax2.plot(topic_range, exclusivity_scores, 'o--', color=color2, label='Exclusivity')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "for i, txt in enumerate(exclusivity_scores):\n",
    "    ax2.annotate(f'{txt:.3f}', (list(topic_range)[i], exclusivity_scores[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "fig.suptitle('Model Selection: Coherence vs. Exclusivity', fontsize=16)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "print(\"\\nShowing plot. Please close the plot window to continue...\")\n",
    "plt.show()\n",
    "\n",
    "# --- User Input for Final Model ---\n",
    "while True:\n",
    "    try:\n",
    "        NUM_TOPICS = int(input(f\"\\nBased on the plot, please enter the final number of topics to use (e.g., from {list(topic_range)}): \"))\n",
    "        if NUM_TOPICS in topic_range:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid input. Please choose a number from the tested range: {list(topic_range)}\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter an integer.\")\n",
    "\n",
    "print(f\"\\nProceeding with {NUM_TOPICS} topics for the final models.\")\n",
    "\n",
    "# =========================================================================== #\n",
    "# 2. FINAL LDA (Latent Dirichlet Allocation)\n",
    "# =========================================================================== #\n",
    "print(\"\\n--- Running Final LDA Model ---\")\n",
    "\n",
    "final_lda_model = LdaModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=NUM_TOPICS, passes=20, random_state=42, alpha='auto', eta='auto')\n",
    "print(\"Assigning dominant topics to sentences...\")\n",
    "df['Topic'] = [max(final_lda_model[doc], key=lambda x: x[1])[0] if doc else -1 for doc in tfidf_corpus]\n",
    "df['Topic_Probability'] = [max(final_lda_model[doc], key=lambda x: x[1])[1] if doc else 0.0 for doc in tfidf_corpus]\n",
    "\n",
    "# --- Consolidating LDA results into single files ---\n",
    "print(\"Exporting consolidated LDA results...\")\n",
    "all_topic_words = []\n",
    "all_rep_sentences = []\n",
    "\n",
    "for topic_id in range(final_lda_model.num_topics):\n",
    "    topic_words = final_lda_model.show_topic(topic_id, topn=NUM_TOPIC_WORDS)\n",
    "    words_df = pd.DataFrame(topic_words, columns=['Word', 'Weight'])\n",
    "    words_df['Topic_ID'] = topic_id\n",
    "    all_topic_words.append(words_df)\n",
    "\n",
    "    rep_docs_df = df[df['Topic'] == topic_id].sort_values(by='Topic_Probability', ascending=False).head(NUM_REPRESENTATIVE_SENTENCES)\n",
    "    all_rep_sentences.append(rep_docs_df)\n",
    "\n",
    "final_words_df = pd.concat(all_topic_words, ignore_index=True)\n",
    "final_sentences_df = pd.concat(all_rep_sentences, ignore_index=True)\n",
    "\n",
    "# --- NEW: Explicitly select and order columns for the sentences CSV ---\n",
    "# Define the columns you want in the final output file.\n",
    "# This ensures 'Year' and 'CEO_Name' are included.\n",
    "columns_to_save = [\n",
    "    original_sentence_col,\n",
    "    'Topic',\n",
    "    'Topic_Probability',\n",
    "    'person_name',\n",
    "    'Year'\n",
    "]\n",
    "# Filter the DataFrame to only include these columns before saving.\n",
    "final_sentences_df_to_save = final_sentences_df[columns_to_save]\n",
    "\n",
    "final_words_df.to_csv(os.path.join(OUTPUT_DIR, 'lda_results', 'lda_all_topic_words.csv'), index=False)\n",
    "final_sentences_df_to_save.to_csv(os.path.join(OUTPUT_DIR, 'lda_results', 'lda_all_representative_sentences.csv'), index=False)\n",
    "print(f\"âœ… Final LDA results exported to single files in '{os.path.join(OUTPUT_DIR, 'lda_results')}' directory.\")\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "# 3. LSI & NMF (For Validation)\n",
    "# =========================================================================== #\n",
    "print(\"\\n--- Running Validation Models (LSI & NMF) ---\")\n",
    "lsi_model = LsiModel(corpus=tfidf_corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "nmf_model = Nmf(corpus=tfidf_corpus, id2word=dictionary, num_topics=NUM_TOPICS, random_state=42)\n",
    "lsi_topics = [{'Topic_ID': i, 'Word': w, 'Weight': s} for i in range(NUM_TOPICS) for w, s in lsi_model.show_topic(i, topn=NUM_TOPIC_WORDS)]\n",
    "nmf_topics = [{'Topic_ID': i, 'Word': w, 'Weight': s} for i in range(NUM_TOPICS) for w, s in nmf_model.show_topic(i, topn=NUM_TOPIC_WORDS)]\n",
    "pd.DataFrame(lsi_topics).to_csv(os.path.join(OUTPUT_DIR, 'validation_models', 'lsi_topics.csv'), index=False)\n",
    "pd.DataFrame(nmf_topics).to_csv(os.path.join(OUTPUT_DIR, 'validation_models', 'nmf_topics.csv'), index=False)\n",
    "print(f\"âœ… LSI and NMF validation topics exported.\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All processes complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9fcc85",
   "metadata": {},
   "source": [
    "Topic Modelling with 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LdaModel\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = 'preprocessed_sentences_final.csv'\n",
    "OUTPUT_DIR = 'topic_model_outputs'\n",
    "NUM_TOPIC_WORDS = 15 # For file exports\n",
    "\n",
    "# --- Create Output Directories ---\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'lda_results')):\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, 'lda_results'))\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = df\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{INPUT_FILE}' not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "all_preprocessed_sentences_tokenized = [str(s).split() for s in df['preprocessed_tokens_str']]\n",
    "\n",
    "# --- Prepare Gensim Corpus ---\n",
    "print(\"Preparing Gensim corpus...\")\n",
    "dictionary = Dictionary(all_preprocessed_sentences_tokenized)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.9, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in all_preprocessed_sentences_tokenized]\n",
    "tfidf_model = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "\n",
    "# Use the number of topics you determined from your tuning phase.\n",
    "NUM_TOPICS = 5\n",
    "print(f\"\\nProceeding with {NUM_TOPICS} topics for the final models.\")\n",
    "\n",
    "# =========================================================================== #\n",
    "# FINAL LDA (Latent Dirichlet Allocation)\n",
    "# =========================================================================== #\n",
    "print(\"\\n--- Running Final LDA Model ---\")\n",
    "\n",
    "final_lda_model = LdaModel(\n",
    "    corpus=tfidf_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    passes=20,\n",
    "    random_state=42,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# --- Assign Topics to DataFrame ---\n",
    "print(\"Assigning dominant topics to sentences...\")\n",
    "# (This part can be commented out if you only need to see the topics)\n",
    "# df['Topic'] = [max(final_lda_model[doc], key=lambda x: x[1])[0] if doc else -1 for doc in tfidf_corpus]\n",
    "# ...\n",
    "\n",
    "# --- Display and Export LDA Results ---\n",
    "print(\"\\n--- LDA Model Results (Gensim Format) ---\")\n",
    "\n",
    "# --- MODIFIED: Display Top 10 Topic Keywords in Gensim Format ---\n",
    "# You can also use the built-in function for a quick view:\n",
    "# final_lda_model.print_topics()\n",
    "\n",
    "for topic_id in range(final_lda_model.num_topics):\n",
    "    topic_words = final_lda_model.show_topic(topic_id, topn=10)\n",
    "    # Build the string in the desired format\n",
    "    topic_string = \" + \".join([f'{weight:.3f}*\"{word}\"' for word, weight in topic_words])\n",
    "    print(f\"Topic #{topic_id}: {topic_string}\")\n",
    "# --- End of Modified Section ---\n",
    "\n",
    "# --- Exporting to files (optional, can be commented out) ---\n",
    "# all_topic_words = []\n",
    "# for topic_id in range(final_lda_model.num_topics):\n",
    "#     topic_words_for_export = final_lda_model.show_topic(topic_id, topn=NUM_TOPIC_WORDS)\n",
    "#     words_df = pd.DataFrame(topic_words_for_export, columns=['Word', 'Weight'])\n",
    "#     words_df['Topic_ID'] = topic_id\n",
    "#     all_topic_words.append(words_df)\n",
    "#\n",
    "# final_words_df = pd.concat(all_topic_words, ignore_index=True)\n",
    "# final_words_df.to_csv(os.path.join(OUTPUT_DIR, 'lda_results', 'lda_all_topic_words.csv'), index=False)\n",
    "# print(f\"\\nâœ… Final LDA topic words also exported to a file.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
