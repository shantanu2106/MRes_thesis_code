{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22035bc",
   "metadata": {},
   "source": [
    "Preprocessing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from nltk.corpus import stopwords\n",
    "import nltk # Added import for NLTK download check\n",
    "\n",
    "# --- Setup: Load spaCy Model and Stopwords ---\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model 'en_core_web_lg' not found. Please run: python -m spacy download en_core_web_lg\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords not found. Downloading...\")\n",
    "    nltk.download('stopwords')\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# --- Helper Functions (No changes needed here) ---\n",
    "def _is_token_related_to_target_person_as_subject_or_object(\n",
    "    token, sentence_person_entities: List[spacy.tokens.Span], subject_deps: Set[str], object_deps: Set[str]\n",
    ") -> bool:\n",
    "    for child in token.children:\n",
    "        if child.dep_ in subject_deps:\n",
    "            for person_ent in sentence_person_entities:\n",
    "                if child.i >= person_ent.start and child.i < person_ent.end:\n",
    "                    return True\n",
    "    for child in token.children:\n",
    "        if child.dep_ in object_deps:\n",
    "            for person_ent in sentence_person_entities:\n",
    "                if child.i >= person_ent.start and child.i < person_ent.end:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def _is_adjective_modifying_target_person(\n",
    "    adj_token, sentence_person_entities: List[spacy.tokens.Span], adjectival_deps: Set[str]\n",
    ") -> bool:\n",
    "    if adj_token.dep_ in adjectival_deps:\n",
    "        head_token = adj_token.head\n",
    "        for person_ent in sentence_person_entities:\n",
    "            if head_token.i >= person_ent.start and head_token.i < person_ent.end:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def _is_noun_describing_target_person(\n",
    "    noun_token, sentence_person_entities: List[spacy.tokens.Span]\n",
    ") -> bool:\n",
    "    if noun_token.dep_ == \"appos\":\n",
    "        head_token = noun_token.head\n",
    "        for person_ent in sentence_person_entities:\n",
    "            if head_token.i >= person_ent.start and head_token.i < person_ent.end:\n",
    "                return True\n",
    "    if noun_token.dep_ == \"attr\" and noun_token.head.pos_ == \"VERB\":\n",
    "        verb_head = noun_token.head\n",
    "        for child_of_verb in verb_head.children:\n",
    "            if child_of_verb.dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
    "                for person_ent in sentence_person_entities:\n",
    "                    if child_of_verb.i >= person_ent.start and child_of_verb.i < person_ent.end:\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "# CHANGED: The function now accepts a DataFrame instead of a list of strings.\n",
    "def run_preprocessing_pipeline(\n",
    "    articles_df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    year_column: str,\n",
    "    my_target_people: List[str],\n",
    "    my_stopwords: Set[str],\n",
    "    min_sentence_length: int = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs the full preprocessing pipeline on raw articles from a DataFrame.\n",
    "    Returns a DataFrame containing original sentence, preprocessed sentence, and year.\n",
    "    \"\"\"\n",
    "    print(\"--- Initial Sentence Extraction and Robust Deduplication ---\")\n",
    "    \n",
    "    # CHANGED: Iterate through the DataFrame to access text and year together.\n",
    "    all_raw_sentences_with_metadata = []\n",
    "    for index, row in articles_df.iterrows():\n",
    "        article_text = str(row[text_column])\n",
    "        year = row[year_column] # Get the year from the same row.\n",
    "        doc = nlp(article_text)\n",
    "        for sent in doc.sents:\n",
    "            # Store the sentence text and its associated year as a tuple.\n",
    "            all_raw_sentences_with_metadata.append((sent.text, year))\n",
    "\n",
    "    print(f\"Total sentences extracted: {len(all_raw_sentences_with_metadata)}\")\n",
    "\n",
    "    # CHANGED: Deduplication now works with tuples (sentence, year).\n",
    "    unique_cleaned_sentences_with_metadata = []\n",
    "    seen_exact_fingerprints = set()\n",
    "    \n",
    "    for sent_text, year in all_raw_sentences_with_metadata:\n",
    "        normalized_words_for_fingerprint = sorted([\n",
    "            token.text.lower() for token in nlp(sent_text)\n",
    "            if not token.is_punct and not token.is_digit and token.text.strip() != \"\"\n",
    "        ])\n",
    "        string_fingerprint = \" \".join(normalized_words_for_fingerprint)\n",
    "\n",
    "        if string_fingerprint in seen_exact_fingerprints:\n",
    "            continue\n",
    "        \n",
    "        seen_exact_fingerprints.add(string_fingerprint)\n",
    "        # Keep the sentence and its year together.\n",
    "        unique_cleaned_sentences_with_metadata.append((sent_text.strip(), year))\n",
    "\n",
    "    print(f\"Total unique sentences after deduplication: {len(unique_cleaned_sentences_with_metadata)}\")\n",
    "\n",
    "    print(\"\\nStarting detailed preprocessing with dependency parsing...\")\n",
    "    \n",
    "    target_last_name_to_full_name = {name.split()[-1].lower(): name for name in my_target_people}\n",
    "    adjectival_deps = {\"amod\"}\n",
    "    subject_deps = {\"nsubj\", \"nsubjpass\"}\n",
    "    object_deps = {\"dobj\", \"pobj\"}\n",
    "    adverbial_deps = {\"advmod\"}\n",
    "    temp_person_centric_sentences_data = []\n",
    "\n",
    "    # CHANGED: Loop through unique sentences *and* their associated years.\n",
    "    for raw_sentence_text, year in unique_cleaned_sentences_with_metadata:\n",
    "        doc = nlp(raw_sentence_text)\n",
    "        sentence = list(doc.sents)[0] \n",
    "        persons_in_this_sentence = set()\n",
    "        sentence_person_entities = []\n",
    "\n",
    "        for ent in sentence.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                ent_last_name = ent.text.split()[-1].lower()\n",
    "                if ent_last_name in target_last_name_to_full_name:\n",
    "                    canonical_name = target_last_name_to_full_name[ent_last_name]\n",
    "                    persons_in_this_sentence.add(canonical_name)\n",
    "                    sentence_person_entities.append(ent)\n",
    "\n",
    "        if not persons_in_this_sentence:\n",
    "            continue\n",
    "\n",
    "        processed_tokens = []\n",
    "        # (Rest of the token processing logic is the same)\n",
    "        for token in sentence:\n",
    "            lemma = token.lemma_.lower()\n",
    "            clean_lemma = re.sub(r'[^a-z]', '', lemma)\n",
    "            if not clean_lemma or clean_lemma in my_stopwords or token.is_punct or token.is_digit:\n",
    "                continue\n",
    "            if token.pos_ == \"ADJ\" and _is_adjective_modifying_target_person(token, sentence_person_entities, adjectival_deps):\n",
    "                processed_tokens.append(clean_lemma)\n",
    "            elif token.pos_ == \"VERB\" and _is_token_related_to_target_person_as_subject_or_object(token, sentence_person_entities, subject_deps, object_deps):\n",
    "                processed_tokens.append(clean_lemma)\n",
    "            elif token.pos_ == \"ADV\" and token.dep_ in adverbial_deps:\n",
    "                head_token = token.head\n",
    "                if head_token.pos_ == \"VERB\" and _is_token_related_to_target_person_as_subject_or_object(head_token, sentence_person_entities, subject_deps, object_deps):\n",
    "                    processed_tokens.append(clean_lemma)\n",
    "                elif head_token.pos_ == \"ADJ\" and _is_adjective_modifying_target_person(head_token, sentence_person_entities, adjectival_deps):\n",
    "                    processed_tokens.append(clean_lemma)\n",
    "            elif token.pos_ in {\"NOUN\", \"PROPN\"} and _is_noun_describing_target_person(token, sentence_person_entities):\n",
    "                processed_tokens.append(clean_lemma)\n",
    "\n",
    "        if len(processed_tokens) >= min_sentence_length:\n",
    "            preprocessed_sentence_str = \" \".join(processed_tokens)\n",
    "            for person_name in persons_in_this_sentence:\n",
    "                # CHANGED: Add the 'Year' to the dictionary here.\n",
    "                temp_person_centric_sentences_data.append({\n",
    "                    'person_name': person_name,\n",
    "                    'original_sentence': raw_sentence_text,\n",
    "                    'preprocessed_tokens_str': preprocessed_sentence_str,\n",
    "                    'Year': year \n",
    "                })\n",
    "    \n",
    "    final_preprocessed_data = []\n",
    "    seen_final_original_sentences_for_df = set()\n",
    "\n",
    "    for item in temp_person_centric_sentences_data:\n",
    "        if item['original_sentence'] not in seen_final_original_sentences_for_df:\n",
    "            seen_final_original_sentences_for_df.add(item['original_sentence'])\n",
    "            final_preprocessed_data.append(item)\n",
    "\n",
    "    df = pd.DataFrame(final_preprocessed_data)\n",
    "    print(\"\\nPreprocessing complete. Data prepared for export.\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Load your main DataFrame here ---\n",
    "    # This assumes 'df_filtered' is already loaded and contains the necessary columns.\n",
    "    # For demonstration, let's create a dummy df_filtered.\n",
    "    # Replace this with loading your actual data, e.g., df_filtered = pd.read_csv('your_data.csv')\n",
    "\n",
    "    # --- Configuration ---\n",
    "    my_target_people = df_filtered['CEO_Name'].unique()\n",
    "    my_stopwords = nltk_stopwords.union(custom_stopwords)\n",
    "\n",
    "    # CHANGED: Call the function with the DataFrame and column names.\n",
    "    preprocessed_df = run_preprocessing_pipeline(\n",
    "        articles_df=df_filtered,\n",
    "        text_column='extracted_text_unicode',\n",
    "        year_column='Year',\n",
    "        my_target_people=my_target_people,\n",
    "        my_stopwords=my_stopwords,\n",
    "    )\n",
    "\n",
    "    # The resulting DataFrame now includes the 'Year' column.\n",
    "    print(\"\\nPreview of the final preprocessed DataFrame:\")\n",
    "    print(preprocessed_df.head())\n",
    "\n",
    "    # Save the DataFrame for use in other code blocks\n",
    "    preprocessed_df.to_csv('preprocessed_sentences_final.csv', index=False)\n",
    "    print(\"\\nPreprocessed sentences saved to 'preprocessed_sentences_final.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import necessary libraries\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Define the model name to be used.\n",
    "# en_core_web_lg is a large English model with word vectors.\n",
    "SPACY_MODEL = \"en_core_web_lg\"\n",
    "\n",
    "# 2. Load the spaCy language model\n",
    "# This block checks if the model is installed and downloads it if necessary.\n",
    "# The model contains the vocabulary, syntax, and other data needed to process text.\n",
    "print(f\"Loading spaCy model '{SPACY_MODEL}'...\")\n",
    "try:\n",
    "    nlp = spacy.load(SPACY_MODEL)\n",
    "except OSError:\n",
    "    print(f\"Model '{SPACY_MODEL}' not found. Downloading...\")\n",
    "    # The spacy.cli.download function is used to download models from the command line,\n",
    "    # but we can call it from Python as well.\n",
    "    spacy.cli.download(SPACY_MODEL)\n",
    "    nlp = spacy.load(SPACY_MODEL)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# 3. The text to be analyzed\n",
    "text = \"The pandemic forced Mr. Woods to change direction.\"\n",
    "\n",
    "# 4. Process the text with the spaCy NLP pipeline\n",
    "# This creates a `Doc` object, which is a container for a sequence of tokens\n",
    "# and all of their linguistic annotations (like part-of-speech tags and dependencies).\n",
    "doc = nlp(text)\n",
    "\n",
    "# 5. Define functions to start the servers\n",
    "# We run each server in a separate thread so they can run at the same time.\n",
    "def serve_dependency_parse():\n",
    "    \"\"\"Serves the dependency parse visualization.\"\"\"\n",
    "    print(\"\\nStarting server for Dependency Parse (dep)...\")\n",
    "    # style='dep' is for dependency parsing\n",
    "    displacy.serve(doc, style='dep', auto_select_port=True)\n",
    "\n",
    "def serve_ner():\n",
    "    \"\"\"Serves the Named Entity Recognition visualization.\"\"\"\n",
    "    print(\"\\nStarting server for Named Entity Recognition (ent)...\")\n",
    "    # style='ent' is for Named Entity Recognition\n",
    "    displacy.serve(doc, style='ent', auto_select_port=True)\n",
    "\n",
    "# 6. Create and start the threads\n",
    "dep_thread = threading.Thread(target=serve_dependency_parse)\n",
    "ner_thread = threading.Thread(target=serve_ner)\n",
    "\n",
    "# Set threads as daemon so they will exit when the main program exits\n",
    "dep_thread.daemon = True\n",
    "ner_thread.daemon = True\n",
    "\n",
    "dep_thread.start()\n",
    "ner_thread.start()\n",
    "\n",
    "# 7. Keep the main script running\n",
    "print(\"\\nTwo servers are starting on different ports.\")\n",
    "print(\"Copy the URLs from the console and open them in your browser.\")\n",
    "print(\"Stop the script with Ctrl+C in your terminal to shut down the servers.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nShutting down servers.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
